# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -march=aarch64 -run-pass=legalizer -verify-machineinstrs %s -o - | FileCheck %s
---
name:            test_memcpy
tracksRegLiveness: true
body:             |
  bb.1:
    liveins: $w2, $x0, $x1

    ; CHECK-LABEL: name: test_memcpy
    ; CHECK: liveins: $w2, $x0, $x1
    ; CHECK: [[COPY:%[0-9]+]]:_(p0) = COPY $x0
    ; CHECK: [[COPY1:%[0-9]+]]:_(p0) = COPY $x1
    ; CHECK: [[COPY2:%[0-9]+]]:_(s32) = COPY $w2
    ; CHECK: [[ZEXT:%[0-9]+]]:_(s64) = G_ZEXT [[COPY2]](s32)
    ; CHECK: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK: $x0 = COPY [[COPY]](p0)
    ; CHECK: $x1 = COPY [[COPY1]](p0)
    ; CHECK: $x2 = COPY [[ZEXT]](s64)
    ; CHECK: BL &memcpy, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $x0, implicit $x1, implicit $x2
    ; CHECK: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK: RET_ReallyLR
    %0:_(p0) = COPY $x0
    %1:_(p0) = COPY $x1
    %2:_(s32) = COPY $w2
    %4:_(s1) = G_CONSTANT i1 false
    %3:_(s64) = G_ZEXT %2(s32)
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.memcpy), %0(p0), %1(p0), %3(s64), %4(s1)
    RET_ReallyLR

...
---
name:            test_memmove
tracksRegLiveness: true
body:             |
  bb.1:
    liveins: $w2, $x0, $x1

    ; CHECK-LABEL: name: test_memmove
    ; CHECK: liveins: $w2, $x0, $x1
    ; CHECK: [[COPY:%[0-9]+]]:_(p0) = COPY $x0
    ; CHECK: [[COPY1:%[0-9]+]]:_(p0) = COPY $x1
    ; CHECK: [[COPY2:%[0-9]+]]:_(s32) = COPY $w2
    ; CHECK: [[ZEXT:%[0-9]+]]:_(s64) = G_ZEXT [[COPY2]](s32)
    ; CHECK: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK: $x0 = COPY [[COPY]](p0)
    ; CHECK: $x1 = COPY [[COPY1]](p0)
    ; CHECK: $x2 = COPY [[ZEXT]](s64)
    ; CHECK: BL &memmove, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $x0, implicit $x1, implicit $x2
    ; CHECK: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK: RET_ReallyLR
    %0:_(p0) = COPY $x0
    %1:_(p0) = COPY $x1
    %2:_(s32) = COPY $w2
    %4:_(s1) = G_CONSTANT i1 false
    %3:_(s64) = G_ZEXT %2(s32)
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.memmove), %0(p0), %1(p0), %3(s64), %4(s1)
    RET_ReallyLR

...
---
name:            test_memset
tracksRegLiveness: true
body:             |
  bb.1:
    liveins: $w1, $w2, $x0

    ; CHECK-LABEL: name: test_memset
    ; CHECK: liveins: $w1, $w2, $x0
    ; CHECK: [[COPY:%[0-9]+]]:_(p0) = COPY $x0
    ; CHECK: [[COPY1:%[0-9]+]]:_(s32) = COPY $w1
    ; CHECK: [[COPY2:%[0-9]+]]:_(s32) = COPY $w2
    ; CHECK: [[ZEXT:%[0-9]+]]:_(s64) = G_ZEXT [[COPY2]](s32)
    ; CHECK: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK: $x0 = COPY [[COPY]](p0)
    ; CHECK: [[COPY3:%[0-9]+]]:_(s32) = COPY [[COPY1]](s32)
    ; CHECK: $w1 = COPY [[COPY3]](s32)
    ; CHECK: $x2 = COPY [[ZEXT]](s64)
    ; CHECK: BL &memset, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $x0, implicit $w1, implicit $x2
    ; CHECK: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK: RET_ReallyLR
    %0:_(p0) = COPY $x0
    %1:_(s32) = COPY $w1
    %2:_(s32) = COPY $w2
    %5:_(s1) = G_CONSTANT i1 false
    %3:_(s8) = G_TRUNC %1(s32)
    %4:_(s64) = G_ZEXT %2(s32)
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.memset), %0(p0), %3(s8), %4(s64), %5(s1)
    RET_ReallyLR

...
